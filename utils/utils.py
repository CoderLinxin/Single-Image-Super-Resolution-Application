import random

import torchvision.transforms.functional as FT
from torchvision.transforms.functional import InterpolationMode
from PIL import Image
from torch import nn
from torch import Tensor
import torch
import collections.abc
from itertools import repeat
import math
import numpy as np


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py
    # Cut & paste from Pytorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            'mean is more than 2 std from [a, b] in nn.init.trunc_normal_. '
            'The distribution of values may be incorrect.',
            stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        low = norm_cdf((a - mean) / std)
        up = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [low, up], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * low - 1, 2 * up - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution.

    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py

    The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return x
        return tuple(repeat(x, n))

    return parse


to_1tuple = _ntuple(1)
to_2tuple = _ntuple(2)
to_3tuple = _ntuple(3)
to_4tuple = _ntuple(4)
to_ntuple = _ntuple


class AverageMeter(object):
    """
    跟踪记录类，用于统计一组数据的平均值、累加和、数据个数.
    """

    def __init__(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


def convert_image(img: Image, source: str, target: str, is_lr, is_lr_amplify, scaling_factor, is_test: bool = False):
    """
    转换图像格式.

    :参数 img: 输入图像
    :参数 source: 数据源格式, 共有3种
                   (1) 'pil' (PIL图像 [0,255])
                   (2) '[0,1]' (Tensor)
                   (3) '[-1,1]' (Tensor 但是数值范围为 -1~1)
    :参数 target: 数据目标格式, 共5种
                   (1) 'pil' (PIL图像 [0,255])
                   (2) '[0,1]' (Tensor)
                   (3) '[-1,1]' (Tensor 但是数值范围为 -1~1)
                   (4) 'y-channel' (亮度通道 Y，采用 YCbCr 颜色空间, 用于计算 PSNR 和 SSIM)
    :参数 is_test: 测试阶段 rgb 转换到 y 通道的算法有所变化

    :返回: 转换后的图像
    """
    assert source in {'pil', '[0,1]', '[-1,1]'}, "无法转换图像源格式 %s!" % source
    assert target in {
        'pil', '[0,255]', '[0,1]', '[-1,1]', 'y-channel'
    }, "无法转换图像目标格式t %s!" % target

    # 先统一将源图像转换为 [0,1] 格式
    if source == 'pil':
        # 把一个取值范围是 [0,255] 的 [H,W,C] PIL.Image 转换成取值范围是 [0,1.0] 的 [C,H,W] Tensor，
        img = FT.to_tensor(img)  # 等价于调用 transforms.ToTensor()
    elif source == '[0,1]':
        pass  # 已经在[0,1]范围内无需处理
    elif source == '[-1,1]':
        img = (img + 1.) / 2.

    # # 如果需要得到的是 crop_size // scaling_factor 大小的 lr 图像，则还需要进行双三次插值下采样
    # if is_lr:
    #     img = imresize(img, 1 / scaling_factor)
    #
    # # 如果需要得到的是 crop_size 大小的 lr 图像，则还需要进行双三次插值上采样
    # if is_lr_amplify:
    #     img = imresize(img, scaling_factor)

    # 再从 [0,1] 转换至目标格式
    if target == 'pil':
        # 把一个取值范围是 [0,1.0] 的 [C,H,W] Tensor 转换成取值范围是 [0,255] 的 [H,W,C] PIL.Image
        img = FT.to_pil_image(img)  # 等价于调用 transforms.ToPILImage()
    elif target == '[0,255]':
        img = 255. * img
    elif target == '[0,1]':
        pass  # 无需处理
    elif target == '[-1,1]':
        img = 2. * img - 1.
    # 注意这是根据 [0,1] 的 RGB 图片进行转换的 y 通道图片
    elif target == 'y-channel':  # [n,c,h,w] -> [n,h,w]
        if len(img.shape) == 4:
            # Y = 0.257 * R + 0.564 * G + 0.098 * B + 16
            # Y = (65.738 * R +  129.057 * G + 25.064 * B) / 256 + 16
            # 使用第二种写法更精确(第一个公式存在四舍五入)
            # if not is_test:
            #     img = 16. + (65.738 * img[:, 0, :, :] + 129.057 * img[:, 1, :, :] + 25.064 * img[:, 2, :, :]) / 256.
            # else:
            #     img = 16. / 255 + (
            #             65.738 * img[:, 0, :, :] + 129.057 * img[:, 1, :, :] + 25.064 * img[:, 2, :, :]) / 256.
            img = 16. / 255 + (65.738 * img[:, 0, :, :] + 129.057 * img[:, 1, :, :] + 25.064 * img[:, 2, :, :]) / 256.
        elif len(img.shape) == 3:
            # if not is_test:
            #     img = 16. + (65.738 * img[0, :, :] + 129.057 * img[1, :, :] + 25.064 * img[2, :, :]) / 256.
            # else:
            #     img = 16. / 255 + (65.738 * img[0, :, :] + 129.057 * img[1, :, :] + 25.064 * img[2, :, :]) / 256.
            img = 16. / 255 + (65.738 * img[0, :, :] + 129.057 * img[1, :, :] + 25.064 * img[2, :, :]) / 256.

    return img


class ImageTransforms(object):
    """
    图像变换.
    """

    def __init__(
            self,
            split: str,
            crop_size: int,
            scaling_factor: int,
            img_type: str,
            is_lr: bool,
            is_lr_amplify: bool,
    ):
        """
        :参数 split: 'train' or 'eval|test'
        :参数 crop_size: 指高分辨率图像上的裁剪尺寸
        :参数 scaling_factor: 放大比例
        :参数 img_type: 图像预处理方式
        :参数 is_lr: 对截取的高分辨率图像块进行双三次插值下采样,图像尺寸 crop_size // scaling_factor
        :参数 is_lr_amplify: 对截取的高分辨率图像块进行双三次插值下采样并再次使用双三次插值上采样,图像尺寸 crop_size
        """
        self.split = split
        self.crop_size = crop_size
        self.scaling_factor = scaling_factor
        self.img_type = img_type
        self.is_lr = is_lr
        self.is_lr_amplify = is_lr_amplify

        if self.split == 'train':
            assert self.crop_size % self.scaling_factor == 0, "裁剪尺寸不能被放大比例整除!"
        assert img_type in {'[0,255]', '[0,1]', '[-1,1]'}

    def __call__(
            self,
            img: Image,
            box: tuple[float, float, float, float] = None
    ):
        """
        :参数 img: 由 PIL 库读取的图像
        :参数 box: 直接规定了裁剪的区域是多少
        :返回: 处理后的图像
        """
        result: torch.Tensor = None

        # 训练阶段裁剪高分辨率图片
        if self.split == 'train':
            assert img.width >= self.crop_size and img.height >= self.crop_size, \
                f'img.width or img.height < crop_size, crop fail~'

            # 获取 box
            if box is None:
                # 从原图中随机裁剪一个 crop_size 大小的图像块
                left = random.randint(0, img.width - self.crop_size)
                top = random.randint(0, img.height - self.crop_size)
                right = left + self.crop_size
                bottom = top + self.crop_size
                # crop((left, top, right, bottom))，其中 left、Top 是闭，right、bottom 是开
                # 也就是说裁剪后的图像宽度 = right - left, 裁剪后的图像高度 = bottom - Top
                box = (left, top, right, bottom)
            # 根据 box 作裁剪
            result = img.crop(box)
            # 安全性检查
            assert result.width == self.crop_size and result.height == self.crop_size
        # 验证/测试阶段裁剪高分辨率图片
        elif self.split == 'eval|test':
            if box is None:
                # 从原图中尽可能大地中心裁剪出大小能被 scaling_factor 整除的图像块
                x_remainder = img.width % self.scaling_factor
                y_remainder = img.height % self.scaling_factor
                left = x_remainder // 2
                top = y_remainder // 2
                right = img.width - (x_remainder - left)
                bottom = img.height - (y_remainder - top)
                box = (left, top, right, bottom)

            # 根据 box 作裁剪
            result = img.crop(box)

        # 如果需要得到的是 crop_size // scaling_factor 大小的 lr 图像，则还需要进行双三次插值下采样
        if self.is_lr:
            result = result.resize(
                (result.width // self.scaling_factor,
                 result.height // self.scaling_factor),
                Image.BICUBIC
            )

        # 如果需要得到的是 crop_size 大小的 lr 图像，则还需要进行双三次插值上采样
        if self.is_lr_amplify:
            result = result.resize(
                (result.width * self.scaling_factor,
                 result.height * self.scaling_factor),
                Image.BICUBIC
            )

        # 转换图像
        result = convert_image(result, source='pil', target=self.img_type, is_lr=self.is_lr, is_lr_amplify=self.is_lr_amplify,
                               scaling_factor=self.scaling_factor)

        return result, box


def adjust_learning_rate(optimizer, shrink_factor):
    """
    调整学习率.

    :参数 optimizer: 需要调整的优化器
    :参数 shrink_factor: 调整因子，范围在 (0, 1) 之间，用于乘上原学习率.
    """

    print("\n调整学习率.")
    for param_group in optimizer.param_groups:
        param_group['lr'] = param_group['lr'] * shrink_factor
    print("新的学习率为 %f\n" % (optimizer.param_groups[0]['lr'],))


# 格式化浮点数字符串
def format_str(num: str or float, digit: int = 18, padding='0'):
    """
    Args:
        num: 带格式化的字符串
        digit: 格式化后的字符串总位数
        padding: 位数不足时使用 padding 后填充

    Returns:

    """
    num = str(num)
    if len(num) >= digit: return num

    # 小于 digit 指定的位数时在小数部分补 0
    format_str = padding * digit
    return (num + format_str)[:digit]


def charbonnier_loss(input, target, eps=1e-12, reduction='mean'):
    result = torch.sqrt((input - target) ** 2 + eps)
    if reduction == 'mean':
        result = result.mean()
    elif reduction == 'sum':
        result = result.sum()
    return result


class CharbonnierLoss(nn.Module):
    """Charbonnier loss (one variant of Robust L1Loss, a differentiable
    variant of L1Loss).

    Described in "Deep Laplacian Pyramid Networks for Fast and Accurate
        Super-Resolution".

    Args:
        loss_weight (float): Loss weight for L1 loss. Default: 1.0.
        reduction (str): Specifies the reduction to apply to the output.
            Supported choices are 'none' | 'mean' | 'sum'. Default: 'mean'.
        eps (float): A value used to control the curvature near zero. Default: 1e-12.
    """

    def __init__(self, loss_weight=1.0, reduction='mean', eps=1e-12):
        super(CharbonnierLoss, self).__init__()
        if reduction not in ['none', 'mean', 'sum']:
            raise ValueError(f'Unsupported reduction mode: {reduction}. Supported ones are: none, mean, sum')

        self.loss_weight = loss_weight
        self.reduction = reduction
        self.eps = eps

    def forward(self, input: Tensor, target: Tensor):
        """
        Args:
            input (Tensor): of shape (N, C, H, W). Predicted tensor.
            target (Tensor): of shape (N, C, H, W). Ground truth tensor.
        """
        return self.loss_weight * charbonnier_loss(input, target, eps=self.eps, reduction=self.reduction)


# 获取数据增强的参数
def get_augment_param(hflip=True, rotation=True):
    """Augment: horizontal flips OR rotate (0, 90, 180, 270 degrees).
    使用 水平翻转 + 垂直翻转 + 旋转 90° 可以表达出: 水平翻转 + 旋转 90°、180°、270° 共 8 种变换
    We use vertical flip and transpose for rotation implementation.
    All the images in the list use the same augmentation.

    """
    hflip = hflip and random.random() < 0.5  # 是否水平翻转
    vflip = rotation and random.random() < 0.5  # 是否垂直翻转
    rot90 = rotation and random.random() < 0.5  # 是否旋转 90°

    return hflip, vflip, rot90


def augment(img: torch.Tensor, hflip: bool, vflip: bool, rot90: bool):
    """
    img: (c,h,w)
    """
    assert len(img.shape) == 3
    c, h, w = img.shape
    assert h == w

    if hflip:  # horizontal
        img = FT.hflip(img)
    if vflip:  # vertical
        img = FT.vflip(img)
    if rot90:  # 使用转置(HW)来实现(等于旋转90°加翻转)
        img = img.permute(0, 2, 1)
    return img


def imresize(img, scale, antialiasing=True):
    """imresize function same as MATLAB.

    It now only supports bicubic.
    The same scale applies for both height and width.

    Args:
        img (Tensor | Numpy array):
            Tensor: Input image with shape (c, h, w), [0, 1] range.
            Numpy: Input image with shape (h, w, c), [0, 1] range.
        scale (float): Scale factor. The same scale applies for both height
            and width.
        antialisaing (bool): Whether to apply anti-aliasing when downsampling.
            Default: True.

    Returns:
        Tensor: Output image with shape (c, h, w), [0, 1] range, w/o round.
    """
    squeeze_flag = False
    if type(img).__module__ == np.__name__:  # numpy type
        numpy_type = True
        if img.ndim == 2:
            img = img[:, :, None]
            squeeze_flag = True
        img = torch.from_numpy(img.transpose(2, 0, 1)).float()
    else:
        numpy_type = False
        if img.ndim == 2:
            img = img.unsqueeze(0)
            squeeze_flag = True

    in_c, in_h, in_w = img.size()
    out_h, out_w = math.ceil(in_h * scale), math.ceil(in_w * scale)
    kernel_width = 4
    kernel = 'cubic'

    # get weights and indices
    weights_h, indices_h, sym_len_hs, sym_len_he = calculate_weights_indices(in_h, out_h, scale, kernel, kernel_width,
                                                                             antialiasing)
    weights_w, indices_w, sym_len_ws, sym_len_we = calculate_weights_indices(in_w, out_w, scale, kernel, kernel_width,
                                                                             antialiasing)
    # process H dimension
    # symmetric copying
    img_aug = torch.FloatTensor(in_c, in_h + sym_len_hs + sym_len_he, in_w)
    img_aug.narrow(1, sym_len_hs, in_h).copy_(img)

    sym_patch = img[:, :sym_len_hs, :]
    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(1, inv_idx)
    img_aug.narrow(1, 0, sym_len_hs).copy_(sym_patch_inv)

    sym_patch = img[:, -sym_len_he:, :]
    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(1, inv_idx)
    img_aug.narrow(1, sym_len_hs + in_h, sym_len_he).copy_(sym_patch_inv)

    out_1 = torch.FloatTensor(in_c, out_h, in_w)
    kernel_width = weights_h.size(1)
    for i in range(out_h):
        idx = int(indices_h[i][0])
        for j in range(in_c):
            out_1[j, i, :] = img_aug[j, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_h[i])

    # process W dimension
    # symmetric copying
    out_1_aug = torch.FloatTensor(in_c, out_h, in_w + sym_len_ws + sym_len_we)
    out_1_aug.narrow(2, sym_len_ws, in_w).copy_(out_1)

    sym_patch = out_1[:, :, :sym_len_ws]
    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(2, inv_idx)
    out_1_aug.narrow(2, 0, sym_len_ws).copy_(sym_patch_inv)

    sym_patch = out_1[:, :, -sym_len_we:]
    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()
    sym_patch_inv = sym_patch.index_select(2, inv_idx)
    out_1_aug.narrow(2, sym_len_ws + in_w, sym_len_we).copy_(sym_patch_inv)

    out_2 = torch.FloatTensor(in_c, out_h, out_w)
    kernel_width = weights_w.size(1)
    for i in range(out_w):
        idx = int(indices_w[i][0])
        for j in range(in_c):
            out_2[j, :, i] = out_1_aug[j, :, idx:idx + kernel_width].mv(weights_w[i])

    if squeeze_flag:
        out_2 = out_2.squeeze(0)
    if numpy_type:
        out_2 = out_2.numpy()
        if not squeeze_flag:
            out_2 = out_2.transpose(1, 2, 0)

    return out_2


def cubic(x):
    """cubic function used for calculate_weights_indices."""
    absx = torch.abs(x)
    absx2 = absx ** 2
    absx3 = absx ** 3
    return (1.5 * absx3 - 2.5 * absx2 + 1) * (
        (absx <= 1).type_as(absx)) + (-0.5 * absx3 + 2.5 * absx2 - 4 * absx + 2) * (((absx > 1) *
                                                                                     (absx <= 2)).type_as(absx))


def calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):
    """Calculate weights and indices, used for imresize function.

    Args:
        in_length (int): Input length.
        out_length (int): Output length.
        scale (float): Scale factor.
        kernel_width (int): Kernel width.
        antialisaing (bool): Whether to apply anti-aliasing when downsampling.
    """

    if (scale < 1) and antialiasing:
        # Use a modified kernel (larger kernel width) to simultaneously
        # interpolate and antialias
        kernel_width = kernel_width / scale

    # Output-space coordinates
    x = torch.linspace(1, out_length, out_length)

    # Input-space coordinates. Calculate the inverse mapping such that 0.5
    # in output space maps to 0.5 in input space, and 0.5 + scale in output
    # space maps to 1.5 in input space.
    u = x / scale + 0.5 * (1 - 1 / scale)

    # What is the left-most pixel that can be involved in the computation?
    left = torch.floor(u - kernel_width / 2)

    # What is the maximum number of pixels that can be involved in the
    # computation?  Note: it's OK to use an extra pixel here; if the
    # corresponding weights are all zero, it will be eliminated at the end
    # of this function.
    p = math.ceil(kernel_width) + 2

    # The indices of the input pixels involved in computing the k-th output
    # pixel are in row k of the indices matrix.
    indices = left.view(out_length, 1).expand(out_length, p) + torch.linspace(0, p - 1, p).view(1, p).expand(
        out_length, p)

    # The weights used to compute the k-th output pixel are in row k of the
    # weights matrix.
    distance_to_center = u.view(out_length, 1).expand(out_length, p) - indices

    # apply cubic kernel
    if (scale < 1) and antialiasing:
        weights = scale * cubic(distance_to_center * scale)
    else:
        weights = cubic(distance_to_center)

    # Normalize the weights matrix so that each row sums to 1.
    weights_sum = torch.sum(weights, 1).view(out_length, 1)
    weights = weights / weights_sum.expand(out_length, p)

    # If a column in weights is all zero, get rid of it. only consider the
    # first and last column.
    weights_zero_tmp = torch.sum((weights == 0), 0)
    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):
        indices = indices.narrow(1, 1, p - 2)
        weights = weights.narrow(1, 1, p - 2)
    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):
        indices = indices.narrow(1, 0, p - 2)
        weights = weights.narrow(1, 0, p - 2)
    weights = weights.contiguous()
    indices = indices.contiguous()
    sym_len_s = -indices.min() + 1
    sym_len_e = indices.max() - in_length
    indices = indices + sym_len_s - 1
    return weights, indices, int(sym_len_s), int(sym_len_e)

# if __name__ == '__main__':
#     print(to_2tuple(5))
